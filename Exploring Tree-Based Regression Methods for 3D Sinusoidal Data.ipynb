{"cells":[{"cell_type":"markdown","metadata":{"id":"xige-uz_5Zsv"},"source":["# Assignment 3: Exploring Tree-Based Regression Methods for 3D Sinusoidal Data\n","## DTSC 680: Applied Machine Learning\n","\n","## Name: Kurtis Crowe"]},{"cell_type":"markdown","metadata":{"id":"crAcD2pj5Zsy"},"source":["## Directions and Overview\n","\n","The main purpose of this assignment is for you to gain experience using tree-based methods to solve simple regression problems.  In this assignment, you will fit a `Gradient-Boosted Regression Tree`, a `Random Forest`, and a `Decision Tree` to a noisy 3D sinusoidal data set.  Since these models can be trained very quickly on the supplied data, I want you to first manually adjust hyperparameter values and observe their influence on the model's predictions.  That is, you should manually sweep the hyperparameter space and try to hone in on the optimal hyperparameter values, again, _manually_.  (Yep, that means guess-and-check: pick some values, train the model, observe the prediction curve, repeat.)\n","\n","But wait, there's more! Merely attempting to identify the optimal hyperparameter values is not enough.  Be sure to really get a visceral understanding of how altering a hyperparameter in turn alters the model predictions (i.e. the prediction curve).  This is how you will build your machine learning intuition!\n","\n","So, play around and build some models.  When you are done playing with hyperparameter values, you should try to set these values to the optimal values manually (you're likely going to be _way_ off).  Then, retrain the model.  Next in this assignment, we will perform several grid searches, so you'll be able to compare your \"optimal\" hyperparameter values with those computed from the grid search.\n","\n","We will visualize model predictions for the optimal `Gradient-Boosted Regression Tree`, a `Random Forest`, and `Decision Tree` models that were determined by the grid searches.  Next, you will compute the generalization error on the test set for the three models."]},{"cell_type":"markdown","metadata":{"id":"7s7r0Cd75Zsz"},"source":["## Preliminaries\n","\n","Let's import some common packages:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPy_lGBD5Zsz"},"outputs":[],"source":["# Common imports\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib import cm\n","import numpy as np\n","import pandas as pd\n","%matplotlib inline\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","import os\n","\n","# Where to save the figures\n","PROJECT_ROOT_DIR = \".\"\n","FOLDER = \"figures\"\n","IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, FOLDER)\n","os.makedirs(IMAGES_PATH, exist_ok=True)\n","\n","def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n","    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format=fig_extension, dpi=resolution)\n","    \n","def plot3Ddata(train_df):\n","    fig = plt.figure(figsize=(15, 15))\n","    \n","    # First subplot\n","    ax1 = fig.add_subplot(221, projection='3d')\n","    ax1.scatter3D(train_df['x'], train_df['y'], train_df['z'], c='steelblue')\n","    ax1.set_xlabel('X', color='maroon', labelpad=10)\n","    ax1.set_ylabel('Y', color='maroon', labelpad=10)\n","    ax1.set_zlabel('Z', color='maroon', labelpad=10)\n","    ax1.view_init(0, 89)\n","    ax1.set_xlim(0,14)\n","    ax1.set_yticks([-6,-4,-2,0,2,4,6])\n","\n","    # Second subplot\n","    ax2 = fig.add_subplot(222, projection='3d')\n","    ax2.scatter3D(train_df['x'], train_df['y'], train_df['z'], c='steelblue')\n","    ax2.set_xlabel('X', color='maroon', labelpad=10)\n","    ax2.set_ylabel('Y', color='maroon', labelpad=10)\n","    ax2.set_zlabel('Z', color='maroon', labelpad=10)\n","    ax2.view_init(50, 0)\n","    ax2.set_yticks(train_df['y'])\n","    ax2.set_xlim(0,14)\n","    ax2.set_yticks([-6,-4,-2,0,2,4,6])\n","    \n","    # Third subplot\n","    ax3 = fig.add_subplot(223, projection='3d')\n","    ax3.scatter3D(train_df['x'], train_df['y'], train_df['z'], c='steelblue')\n","    ax3.set_xlabel('X', color='maroon', labelpad=10)\n","    ax3.set_ylabel('Y', color='maroon', labelpad=10)\n","    ax3.set_zlabel('Z', color='maroon', labelpad=10)\n","    ax3.view_init(40, 40)\n","    ax3.set_xlim(0,14)\n","    ax3.set_yticks([-6,-4,-2,0,2,4,6])\n","\n","    # Fourth subplot\n","    ax4 = fig.add_subplot(224, projection='3d')\n","    ax4.scatter3D(train_df['x'], train_df['y'], train_df['z'], c='steelblue')\n","    ax4.set_xlabel('X', color='maroon', labelpad=10)\n","    ax4.set_ylabel('Y', color='maroon', labelpad=10)\n","    ax4.set_zlabel('Z', color='maroon', labelpad=10)\n","    ax4.view_init(30, 20)\n","    ax4.set_xlim(0,14)\n","    ax4.set_yticks([-6,-4,-2,0,2,4,6])\n","\n","    \n","def plotscatter3Ddata(fit_x, fit_y, fit_z, scat_x, scat_y, scat_z):\n","    fig = plt.figure(figsize=(15, 15))\n","    \n","    # First subplot\n","    ax1 = fig.add_subplot(221, projection='3d')\n","    ax1.scatter3D(scat_x, scat_y, scat_z, c='steelblue', label='Training Data')\n","    ax1.plot3D(fit_x, fit_y, fit_z, c='black', label='Model Predictions', alpha=1)\n","    ax1.set_xlabel('X', color='maroon', labelpad=10)\n","    ax1.set_ylabel('Y', color='maroon', labelpad=10)\n","    ax1.set_zlabel('Z', color='maroon', labelpad=10)\n","    ax1.view_init(0, 89)\n","    ax1.set_xlim(0, 14)\n","    ax1.set_yticks([-6, -4, -2, 0, 2, 4, 6])\n","    ax1.set_title('Plot 1')\n","\n","    # Second subplot\n","    ax2 = fig.add_subplot(222, projection='3d')\n","    ax2.scatter3D(scat_x, scat_y, scat_z, c='steelblue', label='Training Data')\n","    ax2.plot3D(fit_x, fit_y, fit_z, c='black', label='Model Predictions', alpha=1)\n","    ax2.set_xlabel('X', color='maroon', labelpad=10)\n","    ax2.set_ylabel('Y', color='maroon', labelpad=10)\n","    ax2.set_zlabel('Z', color='maroon', labelpad=10)\n","    ax2.view_init(50, 0)\n","    ax2.set_xlim(0, 14)\n","    ax2.set_yticks([-6, -4, -2, 0, 2, 4, 6])\n","    ax2.set_title('Plot 2')\n","    \n","    # Third subplot\n","    ax3 = fig.add_subplot(223, projection='3d')\n","    ax3.scatter3D(scat_x, scat_y, scat_z, c='steelblue', label='Training Data')\n","    ax3.plot3D(fit_x, fit_y, fit_z, c='black', label='Model Predictions', alpha=1)\n","    ax3.set_xlabel('X', color='maroon', labelpad=10)\n","    ax3.set_ylabel('Y', color='maroon', labelpad=10)\n","    ax3.set_zlabel('Z', color='maroon', labelpad=10)\n","    ax3.view_init(40, 40)\n","    ax3.set_xlim(0, 14)\n","    ax3.set_yticks([-6, -4, -2, 0, 2, 4, 6])\n","    ax3.set_title('Plot 3')\n","\n","    # Fourth subplot\n","    ax4 = fig.add_subplot(224, projection='3d')\n","    ax4.scatter3D(scat_x, scat_y, scat_z, c='steelblue', label='Training Data')\n","    ax4.plot3D(fit_x, fit_y, fit_z, c='black', label='Model Predictions', alpha=1)\n","    ax4.set_xlabel('X', color='maroon', labelpad=10)\n","    ax4.set_ylabel('Y', color='maroon', labelpad=10)\n","    ax4.set_zlabel('Z', color='maroon', labelpad=10)\n","    ax4.view_init(30, 20)\n","    ax4.set_xlim(0, 14)\n","    ax4.set_yticks([-6, -4, -2, 0, 2, 4, 6])\n","    ax4.set_title('Plot 4')\n","    \n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"cw36L4OY5Zs1"},"source":["# Import and Split Data\n","\n","Complete the following:\n","\n","\n","\n","1. Begin by importing the data from the file called `3DSinusoidal.csv`.  Name the returned DataFrame `data`. \n","\n","2. Call [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with a `test_size` of 20%.  `x` and `y` will be your feature data and `z` will be your response data. Save the output into `X_train`, `X_test`, `z_train`, and `z_test`, respectively.  Specify the `random_state` parameter to be `42` (do this throughout the entire note book)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxwCbzWV5Zs1"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","\n","#importing data calling it data\n","data = pd.read_csv('3DSinusoidal.csv')\n","\n","#splitting the data\n","X = data[['x', 'y']]\n","z = data['z']\n","X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2, random_state=42)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jIx7Hazr5Zs2"},"source":["# Plot Data\n","\n","Simply plot your training data here, so that you know what you are working with.  You must define a function called `plot3Ddata`, which accepts a Pandas DataFrame (composed of 3 spatial coordinates) and uses `scatter3D()` to plot the data.  Use this function to plot only the training data (recall that you don't even want to look at the test set, until you are ready to calculate the generalization error).  You must place the definition of this function in the existing code cell of the above __Preliminaries__ section, and have nothing other than the function invocation in the below cell. \n","\n","You must emulate the graphs shown in the respective sections below. Each of the graphs will have four subplots. Note the various viewing angles that each subplot presents - you can achieve this with the view_init() method. Be sure to label your axes as shown."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SI5c3GLf5Zs2","outputId":"8d527038-d91d-47e9-e87a-ff1c655cf61b"},"outputs":[],"source":["train_df = pd.DataFrame({'x': X_train['x'], 'y': X_train['y'], 'z': z_train})\n","plot3Ddata(train_df)\n"]},{"cell_type":"markdown","metadata":{"id":"7hiBtKva5Zs3"},"source":["## A Quick Note\n","\n","In the following sections you will be asked to plot the training data along with the model's predictions for that data superimposed on it.  You must write a function called `plotscatter3Ddata(fit_x, fit_y, fit_z, scat_x, scat_y, scat_z)` that will plot this figure.  The function accepts six parameters as input, shown in the function signature.  All six input parameters must be NumPy arrays. The Numpy arrays called fit_x and fit_y represent the x and y coordinates from the training data and fit_z represents the model predictions from those coordinates (i.e. the prediction curve). The three Numpy arrays called `scat_x, scat_y,` and  `scat_z` represent the x, y, and z coordinates of the training data.   \n","\n","You must place the definition of the `plotscatter3Ddata(fit_x, fit_y, fit_z, scat_x, scat_y, scat_z)` function in the existing code cell of the above __Preliminaries__ section. (The function header is already there - you must complete the function definition.)\n","\n","You will use the `plotscatter3Ddata()` function in each of the below __Plot Model Predictions for Training Set__ portion of the three __Explore 3D Data__ sections, as well as the __Visualize Optimal Model Predictions__ section.\n","\n","___Important: Below, you will be asked to plot the model's prediction curve along with the training data.  Even if you correctly train the model, you may find that your trendline is very ugly when you first plot it.  If this happens to you, try plotting the model's predictions using a scatter plot rather than a connected line plot.  You should be able to infer the problem and solution with the trendline from examining this new scatter plot of the model's predictions. All of your plots, however, should be connected line plots when submitted. Please refer to the FAQ document for more clarity.___\n","\n","Note: You must use the supplied data and not np.linspace() to create the prediction curve."]},{"cell_type":"markdown","metadata":{"id":"54upMrCW5Zs3"},"source":["# Explore 3D Data: GradientBoostingRegressor\n","\n","Fit a `GradientBoostingRegressor` model to this data.  You must manually assign values to the following hyperparameters.  You should \"play around\" by using different combinations of hyperparameter values to really get a feel for how they affect the model's predictions.  Update the below bullets accordingly.  When you are done playing, set these to the best values you can for submission.  Update the below bullets accordingly.  (It is totally fine if you don't elucidate the optimal values here; however, you will want to make sure your model is not excessively overfitting or underfitting the data.  Do this by examining the prediction curve generated by your model.  You will be graded, more exactly, on the values that you calculate later from performing several rounds of grid searches.)\n","\n","Values \"played around with\":\n"," - `learning_rate = <values>`\n"," - `max_depth = <values>`\n"," - `n_estimators = <values>`\n"," - `random_state = 42`\n","\n","Initial \"best values\":\n"," - `learning_rate = <0.1>`\n"," - `max_depth = <2>`\n"," - `n_estimators = <100>`\n"," - `random_state = 42`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vew4P92e5Zs4"},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingRegressor\n","\n","gbrt =  GradientBoostingRegressor(max_depth=2, n_estimators=100, learning_rate=0.1, random_state=42)\n","gbrt.fit(X_train,z_train) "]},{"cell_type":"markdown","metadata":{"id":"Lxi8t_9u5Zs4"},"source":["### Plot Model Predictions for Training Set\n","\n","Use the `plotscatter3Ddata(fit_x, fit_y, fit_z, scat_x, scat_y, scat_z)` function to plot the data and the prediction curve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMiZiG_55Zs4","outputId":"fa045c86-f5b1-439b-b544-bf0d1fe92048"},"outputs":[],"source":["#create predictions with predict()\n","z_preds = gbrt.predict(X_train)\n","\n","#create new dataframe to sort\n","gbrt_sorted = pd.DataFrame({'X' : X_train.iloc[:, 0],\n","                            'y' : X_train.iloc[:, 1],\n","                            'z' : z_train,\n","                            'z_preds' : z_preds})\n","\n","#sort by X\n","gbrt_sorted = gbrt_sorted.sort_values(by='X')\n","gbrt_sorted = gbrt_sorted.reset_index(drop=True)\n","\n","#convert df to np array\n","gbrt_sorted_np = gbrt_sorted.values\n","\n","#plot using our function\n","plotscatter3Ddata(gbrt_sorted_np[:,0], gbrt_sorted_np[:, 1], gbrt_sorted_np[:,3], gbrt_sorted_np[:,0], gbrt_sorted_np[:, 1], gbrt_sorted_np[:, 2])"]},{"cell_type":"markdown","metadata":{"id":"TCSCFapu5Zs4"},"source":["# Explore 3D Data: RandomForestRegressor\n","\n","Fit a `RandomForestRegressor` model to this data.  You must manually assign values to the following hyperparameters.  You should \"play around\" by using different combinations of hyperparameter values to really get a feel for how they affect the model's predictions.  Update the below bullets accordingly.  When you are done playing, set these to the best values you can for submission.  Update the below bullets accordingly.  (It is totally fine if you don't elucidate the optimal values here; however, you will want to make sure your model is not excessively overfitting or underfitting the data.  Do this by examining the prediction curve generated by your model.  You will be graded, more exactly, on the values that you calculate later from performing several rounds of grid searches.)\n","\n","Values \"played around with\":\n"," - `min_samples_split = <values>`\n"," - `max_depth = <values>`\n"," - `n_estimators = <values>`\n"," - `random_state = 42`\n","\n","Initial \"best values\":\n"," - `min_samples_split = <5>`\n"," - `max_depth = <5>`\n"," - `n_estimators = <100>`\n"," - `random_state = 42`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3hoFvFH5Zs5"},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","rfr =  RandomForestRegressor(max_depth=5, n_estimators=100, min_samples_split=5, random_state=42)\n","rfr.fit(X_train,z_train) "]},{"cell_type":"markdown","metadata":{"id":"MNAxPQVE5Zs5"},"source":["### Plot Model Predictions for Training Set\n","\n","Use the `plotscatter3Ddata(fit_x, fit_y, fit_z, scat_x, scat_y, scat_z)` function to plot the data and the prediction curve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQb518575Zs5","outputId":"cda82d76-7ab6-4a30-f46b-4f226802da4f"},"outputs":[],"source":["#create predictions with predict()\n","z_preds2 = rfr.predict(X_train)\n","\n","#create new dataframe to sort\n","rfr_sorted = pd.DataFrame({'X' : X_train.iloc[:, 0],\n","                            'y' : X_train.iloc[:, 1],\n","                            'z' : z_train,\n","                            'z_preds' : z_preds2})\n","\n","#sort by X\n","rfr_sorted = rfr_sorted.sort_values(by='X')\n","rfr_sorted = rfr_sorted.reset_index(drop=True)\n","\n","#convert df to np array\n","rfr_sorted_np = rfr_sorted.values\n","\n","#plot using plotscatter3Ddata function and numpy array\n","plotscatter3Ddata(rfr_sorted_np[:,0], rfr_sorted_np[:, 1], rfr_sorted_np[:,3], rfr_sorted_np[:,0], rfr_sorted_np[:, 1], rfr_sorted_np[:, 2])"]},{"cell_type":"markdown","metadata":{"id":"hj53pECL5Zs6"},"source":["# Explore 3D Data: DecisionTreeRegressor\n","\n","Fit a `DecisionTreeRegressor` model to this data.  You must manually assign values to the following hyperparameters.  You should \"play around\" by using different combinations of hyperparameter values to really get a feel for how they affect the model's predictions.  Update the below bullets accordingly.  When you are done playing, set these to the best values you can for submission.  Update the below bullets accordingly.  (It is totally fine if you don't elucidate the optimal values here; however, you will want to make sure your model is not excessively overfitting or underfitting the data.  Do this by examining the prediction curve generated by your model.  You will be graded, more exactly, on the values that you calculate later from performing several rounds of grid searches.)\n","\n","Values \"played around with\":\n"," - `splitter = <values>`\n"," - `max_depth = <values>`\n"," - `min_samples_split = <values>`\n"," - `random_state = 42`\n","\n","Initial \"best values\":\n"," - `splitter = <best>`\n"," - `max_depth = <7>`\n"," - `min_samples_split = <2>`\n"," - `random_state = 42`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ktsgWyL5Zs6"},"outputs":[],"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","dtr = DecisionTreeRegressor(splitter='best', max_depth=7, min_samples_split=2, random_state=42)\n","\n","\n","dtr.fit(X_train, z_train)\n"]},{"cell_type":"markdown","metadata":{"id":"ozQSktk85Zs6"},"source":["### Plot Model Predictions for Training Set\n","\n","Use the `plotscatter3Ddata(fit_x, fit_y, fit_z, scat_x, scat_y, scat_z)` function to plot the data and the prediction curve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpbJG0_L5Zs6","outputId":"00fc8d6b-6725-4c2d-ee1f-a6fe3c23566a"},"outputs":[],"source":["#create predictions with predict()\n","z_preds3 = dtr.predict(X_train)\n","\n","#create new dataframe to sort\n","dtr_sorted = pd.DataFrame({'X' : X_train.iloc[:, 0],\n","                            'y' : X_train.iloc[:, 1],\n","                            'z' : z_train,\n","                            'z_preds' : z_preds3})\n","\n","#sort by X\n","dtr_sorted = dtr_sorted.sort_values(by='X')\n","dtr_sorted = dtr_sorted.reset_index(drop=True)\n","\n","#convert df to np array\n","dtr_sorted_np = dtr_sorted.values\n","\n","#plot using plotscatter3Ddata function and numpy array\n","plotscatter3Ddata(dtr_sorted_np[:,0], dtr_sorted_np[:, 1], dtr_sorted_np[:,3], dtr_sorted_np[:,0], dtr_sorted_np[:, 1], dtr_sorted_np[:, 2])"]},{"cell_type":"markdown","metadata":{"id":"G9ldNDC55Zs6"},"source":["# Perform Grid Searches\n","\n","You will perform a series of grid searches, which will yield the optimal hyperparamter values for each of the three model types.  You can compare the values computed by the grid search with the values you manually found earlier.  How do these compare?\n","\n","You must perform a course-grained grid search, with a very broad range of values first.  Then, you perform a second grid search using a tighter range of values centered on those identified in the first grid search.  You may have to use another round of grid searching too (it took me at least three rounds of grid searches per model to ascertain the optimal hyperparameter values below).\n","\n","Note the following:\n","\n","1. Be sure to clearly report the optimal hyperparameters in the designated location after you calculate them!\n","\n","2. You must use `random_state=42` everywhere that it is needed in this notebook.\n","\n","3. You must use grid search to compute the following hyperparameters:\n","\n","   GradientBoostingRegressor:\n","    \n","     - `max_depth = <value>`\n","     - `n_estimators = <value>`\n","     - `learning_rate = <value>`\n","\n","   RandomForestRegressor:\n","    \n","     - `max_depth = <value>`\n","     - `n_estimators = <value>`\n","     - `min_samples_split = <value>`\n","\n","   DecisionTreeRegressor:\n","    \n","     - `splitter = <value>`\n","     - `max_depth = <value>`\n","     - `min_samples_split = <value>`\n","     \n","     \n","4. `learning rate` should be rounded to two decimals.\n","5. The number of cross-folds. Specify `cv=3`\n"]},{"cell_type":"markdown","metadata":{"id":"Ap_dAqmp5Zs7"},"source":["## Perform Individual Model Grid Searches\n","\n","In this section you will perform a series of grid searches to compute the optimal hyperparameter values for each of the three model types."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xaP4psye5Zs7"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","# -----\n","# Coarse-Grained GradientBoostingRegressor GridSearch\n","# -----\n","\n","# gbrt =  GradientBoostingRegressor(max_depth=2, n_estimators=100, learning_rate=0.1, random_state=42)\n","gbr_param_grid = {'max_depth': [1,2,3,4,5,8,16,32],\n","                  'n_estimators': [50, 100, 200, 300,400,500,600,700,800,900,1000],\n","                  'learning_rate': [.01, .05, .10, .30, .40, .50, .75, 1]\n","                  }\n","\n","grid_search_cv = GridSearchCV(GradientBoostingRegressor(random_state=42),\n","                              gbr_param_grid, verbose=1, cv=3)\n","\n","grid_search_cv.fit(X_train, z_train)\n","\n","\n","print(\"Best parameters are: \", grid_search_cv.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# -----\n","# Refined GradientBoostingRegressor GridSearch\n","# -----\n","\n","gbr_param_grid2 = {'max_depth': [ 1, 2, 3],\n","                  'n_estimators': [100, 150, 180, 200, 220, 300,350],\n","                  'learning_rate': [.02, .03, .05, .08, .10,]\n","                  }\n","\n","\n","grid_search_cv = GridSearchCV(GradientBoostingRegressor(random_state=42),\n","                              gbr_param_grid2, verbose=1, cv=3)\n","\n","grid_search_cv.fit(X_train, z_train)\n","\n","\n","print(\"Best parameters are: \", grid_search_cv.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syJbZ_hX5Zs8"},"outputs":[],"source":["# -----\n","# Final GradientBoostingRegressor GridSearch\n","# -----\n","\n","gbr_param_grid3 = {'max_depth': [ 1, 2, 3],\n","                  'n_estimators': [320,330,340,350,360,370,380],\n","                  'learning_rate': [.03, .04, .05, .06, .07,]\n","                  }\n","\n","\n","grid_search_cv = GridSearchCV(GradientBoostingRegressor(random_state=42),\n","                              gbr_param_grid3, verbose=1, cv=3)\n","\n","grid_search_cv.fit(X_train, z_train)\n","\n","\n","print(\"Best parameters are: \", grid_search_cv.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"-gIztjQC5Zs8"},"source":["On this dataset, the optimal model parameters for the `GradientBoostingRegressor` class are:\n","\n","- `learning_rate = <0.05>`\n","- `max_depth = <2>`\n","- `n_estimators = <370>`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1SKVgrc5Zs8","scrolled":true},"outputs":[],"source":["# -----\n","# Coarse-Grained RandomForestRegressor GridSearch\n","# -----\n","#rfr =  RandomForestRegressor(max_depth=5, n_estimators=100, min_samples_split=5, random_state=42)\n","rfr_grid_params = {'max_depth': [1,2,3,5,8,16,32],\n","                  'n_estimators': [50, 100, 200, 500, 1000],\n","                  'min_samples_split': [2, 5, 10, 15, 20]\n","                  }\n","\n","rfr_grid_search = GridSearchCV(RandomForestRegressor(random_state=42),\n","                              rfr_grid_params, verbose=1, cv=3)\n","\n","rfr_grid_search.fit(X_train, z_train)\n","\n","print(\"Best parameters after final grid search:\", rfr_grid_search.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30evzWRU5Zs8","scrolled":true},"outputs":[],"source":["# -----\n","# Refined RandomForestRegressor GridSearch\n","# -----\n","\n","rfr_grid_params2 = {'max_depth': [12, 14, 16, 18, 20],\n","                  'n_estimators': [800,900,1000, 1100, 1200],\n","                  'min_samples_split': [2, 3, 4, 5]\n","                  }\n","\n","rfr_grid_search = GridSearchCV(RandomForestRegressor(random_state=42),\n","                              rfr_grid_params2, verbose=1, cv=3)\n","\n","rfr_grid_search.fit(X_train, z_train)\n","\n","print(\"Best parameters after final grid search:\", rfr_grid_search.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uANYTr8W5Zs8"},"outputs":[],"source":["# -----\n","# Final RandomForestRegressor GridSearch\n","# -----\n","\n","rfr_grid_params3 = {'max_depth': [10, 11, 12, 13, 14],\n","                  'n_estimators': [1060,1080,1100, 1120, 1140],\n","                  'min_samples_split': [2, 3,4]\n","                  }\n","\n","rfr_grid_search = GridSearchCV(RandomForestRegressor(random_state=42),\n","                              rfr_grid_params3, verbose=1, cv=3)\n","\n","rfr_grid_search.fit(X_train, z_train)\n","\n","print(\"Best parameters after final grid search:\", rfr_grid_search.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"mNvOgpvF5Zs8"},"source":["On this dataset, the optimal model parameters for the `RandomForestRegressor` class are:\n","\n","- `max_depth = <11>`\n","- `n_estimators = <1120>`\n","- `min_samples_split = <2>`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QaTsSinw5Zs9"},"outputs":[],"source":["# -----\n","# Coarse-Grained DecisionTreeRegressor GridSearch\n","# -----\n","\n","# dtr = DecisionTreeRegressor(splitter='best', max_depth=7, min_samples_split=2, random_state=42)\n","\n","dtr_param_grid = {\n","    'splitter': [\"best\", \"random\"],  # Splitter criterion\n","    'max_depth': [1, 2, 4, 8, 16, 32],  # Up to 32\n","    'min_samples_split': [2, 5, 10, 15, 20]  # Up to 20\n","}\n","\n","grid_search_dt = GridSearchCV(DecisionTreeRegressor(random_state=42),\n","                              dtr_param_grid, verbose=1, cv=3)\n","\n","\n","\n","grid_search_dt.fit(X_train, z_train)\n","\n","print(\"Best parameters after final grid search:\", grid_search_dt.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mp-SAizT5Zs9"},"outputs":[],"source":["# -----\n","# Final-Grained DecisionTreeRegressor GridSearch\n","# -----\n","\n","dtr_param_grid2 = {\n","    'splitter': [\"best\", \"random\"],  # Splitter criterion\n","    'max_depth': [4, 6, 7, 8, 9, 10,11],  # Up to 32\n","    'min_samples_split': [8, 9, 10, 11, 12]  # Up to 20\n","}\n","\n","grid_search_dt = GridSearchCV(DecisionTreeRegressor(random_state=42),\n","                              dtr_param_grid2, verbose=1, cv=3)\n","\n","\n","\n","grid_search_dt.fit(X_train, z_train)\n","\n","print(\"Best parameters after final grid search:\", grid_search_dt.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"Mv3MZe9W5Zs-"},"source":["On this dataset, the optimal model parameters for the `RandomForestRegressor` class are:\n","\n","- `splitter = <best>`\n","- `max_depth = <7>`\n","- `min_samples_split = <10>`"]},{"cell_type":"markdown","metadata":{"id":"8dyqrwfd5Zs-"},"source":["# Visualize Optimal Model Predictions\n","\n","In the previous section you performed a series of grid searches designed to identify the optimal hyperparameter values for all three models.  Now, use the `best_params_` attribute of the grid search objects from above to create the three optimal models below.  For each model, visualize the models predictions on the training set - this is what we mean by the \"prediction curve\" of the model.\n","\n","### Create Optimal GradientBoostingRegressor Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NfBQC5t5Zs_"},"outputs":[],"source":["optimal_gbr = GradientBoostingRegressor(**grid_search_cv.best_params_, random_state=42)\n","optimal_gbr.fit(X_train, z_train)"]},{"cell_type":"markdown","metadata":{"id":"ksrOMB6R5Zs_"},"source":["### Plot Model Predictions for Training Set\n","\n","Use the `plotscatter3Ddata(fit_x, fit_y, fit_z, scat_x, scat_y, scat_z)` function to plot the data and the prediction curve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ylwqom2B5Zs_"},"outputs":[],"source":["#create predictions with predict()\n","z_preds_gbr = optimal_gbr.predict(X_train)\n","\n","#create new dataframe to sort\n","ogbrt_sorted = pd.DataFrame({'X' : X_train.iloc[:, 0],\n","                            'y' : X_train.iloc[:, 1],\n","                            'z' : z_train,\n","                            'z_preds' : z_preds_gbr})\n","\n","#sort by X\n","ogbrt_sorted = ogbrt_sorted.sort_values(by='X')\n","ogbrt_sorted = ogbrt_sorted.reset_index(drop=True)\n","\n","#convert df to np array\n","ogbrt_sorted_np = ogbrt_sorted.values\n","\n","#plot using our function\n","plotscatter3Ddata(ogbrt_sorted_np[:,0], ogbrt_sorted_np[:, 1], ogbrt_sorted_np[:,3], ogbrt_sorted_np[:,0], ogbrt_sorted_np[:, 1], ogbrt_sorted_np[:, 2])"]},{"cell_type":"markdown","metadata":{"id":"PvmpVlPJ5ZtA"},"source":["### Create Optimal RandomForestRegressor Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"emE4z21k5ZtA"},"outputs":[],"source":["optimal_rf = RandomForestRegressor(**rfr_grid_search.best_params_, random_state=42)\n","\n","optimal_rf.fit(X_train, z_train)"]},{"cell_type":"markdown","metadata":{"id":"9NX3bnzA5ZtA"},"source":["### Plot Model Predictions for Training Set\n","\n","Use the `plotscatter3Ddata(fit_x, fit_y, fit_z, scat_x, scat_y, scat_z)` function to plot the data and the prediction curve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWf4dWfn5ZtA"},"outputs":[],"source":["#create predictions with predict()\n","rf_z_preds = optimal_rf.predict(X_train)\n","\n","#create new dataframe to sort\n","orfr_sorted = pd.DataFrame({'X' : X_train.iloc[:, 0],\n","                            'y' : X_train.iloc[:, 1],\n","                            'z' : z_train,\n","                            'z_preds' : rf_z_preds})\n","\n","#sort by X\n","orfr_sorted = orfr_sorted.sort_values(by='X')\n","orfr_sorted = orfr_sorted.reset_index(drop=True)\n","\n","#convert df to np array\n","orfr_sorted_np = orfr_sorted.values\n","\n","#plot using plotscatter3Ddata function and numpy array\n","plotscatter3Ddata(orfr_sorted_np[:,0], orfr_sorted_np[:, 1], orfr_sorted_np[:,3], orfr_sorted_np[:,0], orfr_sorted_np[:, 1], orfr_sorted_np[:, 2])"]},{"cell_type":"markdown","metadata":{"id":"YRrUPPg85ZtB"},"source":["### Create Optimal DecisionTreeRegressor Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQEEdJfj5ZtB"},"outputs":[],"source":["optimal_dt = DecisionTreeRegressor(**grid_search_dt.best_params_, random_state=42)\n","\n","optimal_dt.fit(X_train, z_train)"]},{"cell_type":"markdown","metadata":{"id":"4nE_SWCO5ZtC"},"source":["### Plot Model Predictions for Training Set\n","\n","Use the `plotscatter3Ddata(fit_x, fit_y, fit_z, scat_x, scat_y, scat_z)` function to plot the data and the prediction curve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfATLj5T5ZtC"},"outputs":[],"source":["#create predictions with predict()\n","dt_z_preds = optimal_dt.predict(X_train)\n","\n","#create new dataframe to sort\n","odt_sorted = pd.DataFrame({'X' : X_train.iloc[:, 0],\n","                            'y' : X_train.iloc[:, 1],\n","                            'z' : z_train,\n","                            'z_preds' : dt_z_preds})\n","\n","#sort by X\n","odt_sorted = odt_sorted.sort_values(by='X')\n","odt_sorted = odt_sorted.reset_index(drop=True)\n","\n","#convert df to np array\n","odt_sorted_np = odt_sorted.values\n","\n","#plot using plotscatter3Ddata function and numpy array\n","plotscatter3Ddata(odt_sorted_np[:,0], odt_sorted_np[:, 1], odt_sorted_np[:,3], odt_sorted_np[:,0], odt_sorted_np[:, 1], odt_sorted_np[:, 2])"]},{"cell_type":"markdown","metadata":{"id":"5x47FyYG5ZtC"},"source":["# Compute Generalization Error\n","\n","Compute the generalization error for each of the optimal models computed above.  Use MSE as the generalization error metric.  Round your answers to four decimal places.  Print the generalization error for all three models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXzVwClZ5ZtC"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","\n","#make prediction and compute MSE of DT\n","y_pred_dt = optimal_dt.predict(X_test)\n","mse_dt = mean_squared_error(z_test, y_pred_dt)\n","\n","#make prediction and compute MSE of RFR\n","y_pred_rf = optimal_rf.predict(X_test)\n","mse_rfr = mean_squared_error(z_test, y_pred_rf)\n","\n","#make prediction and compute MSE of GBR\n","y_pred_gbr = optimal_gbr.predict(X_test)\n","mse_gbr = mean_squared_error(z_test, y_pred_gbr)\n","\n","#print and round Generalization error in order\n","print(\"Generalization error for Gradiant Boosting Regressor:\", round(mse_gbr, 4))\n","print(\"Generalization error for Random Forest Regressor:\", round(mse_rfr, 4))\n","print(\"Generalization error for Decision Tree Regressor:\", round(mse_dt, 4))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
